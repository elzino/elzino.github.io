<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="https://elzino.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://elzino.github.io/" rel="alternate" type="text/html" /><updated>2019-12-30T08:29:33+00:00</updated><id>https://elzino.github.io/</id><title type="html">ELZINO PROJECT</title><subtitle>Small blog for logging my life</subtitle><author><name>Jinho Lee</name><email>elzinomaster@gmail.com</email></author><entry><title type="html">[번역] 딥러닝 분산 학습을 알아보자(Intro to Distributed Deep Learning Systems)</title><link href="https://elzino.github.io/papers/2019-12-30/intro-to-distributed-deep-learning-systems" rel="alternate" type="text/html" title="[번역] 딥러닝 분산 학습을 알아보자(Intro to Distributed Deep Learning Systems)" /><published>2019-12-30T00:00:00+00:00</published><updated>2019-12-30T00:00:00+00:00</updated><id>https://elzino.github.io/papers/2019-12-30/intro-to-distributed-deep-learning-systems</id><content type="html" xml:base="https://elzino.github.io/papers/2019-12-30/intro-to-distributed-deep-learning-systems">&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@Petuum/intro-to-distributed-deep-learning-systems-a2e45c6b8e7&quot;&gt;Intro to Distributed Deep Learning Systems&lt;/a&gt;를 번역한 문서입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;번역이 익숙치 않아 글이 어색할 수 있는 점 이해해주세요.&lt;/p&gt;

&lt;h2 id=&quot;머신러닝-분산-학습이란&quot;&gt;머신러닝 분산 학습이란?&lt;/h2&gt;
&lt;p&gt;일반적으로 머신러닝 분산 학습(distributed machine learning, DML)은 컴퓨터 사이언스 전반의 여러 학문이 포함된 분야입니다. 이론적 영역(통계, &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_learning_theory&quot;&gt;학습이론&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_optimization&quot;&gt;최적화&lt;/a&gt;), 알고리즘, &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;머신러닝&lt;/a&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;딥러닝&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphical_model&quot;&gt;그래프 모델&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method&quot;&gt;커널&lt;/a&gt;), &lt;a href=&quot;https://en.wikipedia.org/wiki/Distributed_computing&quot;&gt;분산 처리&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_data_storage&quot;&gt;기억 장치&lt;/a&gt; 등이 포함됩니다. 이러한 각각의 분야들에서 수많은 주제들이 연구되고 있습니다. 또한 DML(머신러닝 분산 학습)은 빅데이터에 대한 처리 능력때문에 산업에서 폭 넓게 사용되고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;머신러닝-분산-학습으로-해결하려는-문제가-무엇일까&quot;&gt;머신러닝 분산 학습으로 해결하려는 문제가 무엇일까?&lt;/h2&gt;
&lt;p&gt;DML(머신러닝 분산 학습)을 이해하는 가장 쉬운 방법은 연구분야를 4개로 쪼개서 살펴보는 것입니다. 하지만 각각의 분야가 겹칠 수 있다는 점 미리 이해해 주세요.&lt;/p&gt;

&lt;h3 id=&quot;1-통계-최적화-알고리즘을-어떻게-사용할까&quot;&gt;1. 통계, 최적화, 알고리즘을 어떻게 사용할까?&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;대부분의 머신러닝 방법들은 training data에 대한 loss 함수를 최소화 하므로 다음 항목들을 고려해 학습을 진행합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;최적화 과정이 수렴하는데까지 얼마나 걸리는지, 즉 수렴 속도가 얼마인지&lt;/li&gt;
  &lt;li&gt;수렴된 솔루션이 얼마나 좋은지&lt;/li&gt;
  &lt;li&gt;좋은 솔루션을 위해 얼마나 많은 데이터가 필요한 지&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이러한 분야들을 연구하기 위해 연구자들은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_optimization&quot;&gt;최적화 이론&lt;/a&gt;과 &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_learning_theory&quot;&gt;통계적 학습 이론&lt;/a&gt; 같은 이론적 분석을 이용합니다. 하지만 많은 컴퓨팅 자원이 주어지고 병렬, 분산 학습등을 이용해 학습 속도를 올리는 것이 목표인 large-scale의 머신러닝 관점에서 보면 위와 비슷해 보이지만 다른 항목들을 고려하게 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;분산, 병렬 학습을 사용했을 때 우리의 모델이 원래 수렴했던 것과 똑같이 수렴하는 것이 보장되는 지&lt;/li&gt;
  &lt;li&gt;만약 아니라면 원래의 솔루션과 얼마나 다른지, 그리고 본질적인 최적해와 얼마나 다른지&lt;/li&gt;
  &lt;li&gt;좋은 수렴을 위해 다른 가정이나 조건들이 필요한 지&lt;/li&gt;
  &lt;li&gt;분산 학습을 사용하지 않았을 때와 비교해서 얼마나 빠른지, 그리고 어떻게 평가할 것인지&lt;/li&gt;
  &lt;li&gt;좋은 scailabilty와 좋은 수렴을 둘 다 만족하기 위해 어떻게 학습 과정을 디자인 할 것인지&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-분산-학습에-더-적합한-머신러닝-모델-혹은-알고리즘을-어떻게-사용할까&quot;&gt;2. 분산 학습에 더 적합한 머신러닝 모델 혹은 알고리즘을 어떻게 사용할까?&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;이 분야는 새로운 머신러닝 모델을 개발하거나 이미 존재하는 모델을 큰 데이터에 잘 학습하도록 조정하는 것들을 연구합니다.&lt;/p&gt;

&lt;h3 id=&quot;3-분산-머신러닝-모델을-어떻게-실생활에-적용할까&quot;&gt;3. 분산 머신러닝 모델을 어떻게 실생활에 적용할까?&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_vision#Recognition&quot;&gt;이미지 분류&lt;/a&gt;와 같이 특정 모델이나 알고리즘의 scale-up을 요구하는 세부적인 적용 분야가 있습니다. 이러한 문제들에 솔루션들은 대부분 바로 제품에 쓰이기도 합니다.&lt;/p&gt;

&lt;h3 id=&quot;4-머신러닝의-규모를-키우기-위해서-어떻게-분산-병렬-컴퓨터-시스템을-개발할까&quot;&gt;4. 머신러닝의 규모를 키우기 위해서 어떻게 분산, 병렬 컴퓨터 시스템을 개발할까?&lt;/h3&gt;
&lt;p&gt;이 분야는 오히려 더 직관적입니다. 머신러닝 모델이나 알고리즘이 하나의 노드에서 계산 작업을 다 마치지 못했다면 더 많은 노드를 사용해서 분산 시스템을 개발하는 것을 시도해볼 수 있지요. 하지만 더 많은 자원을 사용하기 위해 고려해야 할 사항들이 많습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;일관성(Consistency) : 여러개의 노드들이 동시에 학습을 하고 있다면 어떻게 합쳐야 할까? 예를 하나의 문제를 푸는데 노드마다 갖고 있는 데이터 셋이 다르다면 어떻게 해야할까?&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fault_tolerance&quot;&gt;Fault tolerance&lt;/a&gt; : 클러스터가 1000개의 노드로 구성되어 있는데 만약 그 중 하나가 망가지면 어떻게 할까? 아예 처음부터 다시 시작하지 말고 고칠 수 있는 방법이 있을까?&lt;/li&gt;
  &lt;li&gt;통신(Communication) : 머신러닝은 많은 I/O(디스크 읽기, 쓰기), 데이터 처리 작업을 포함합니다. 다양한 환경에서(single node local disk, distributed file systems, CPU I/O, GPU I/O 등등) 빠른 I/O와 non-blocking 데이터 처리 과정을 가능하게 하는 저장 시스템을 설계할 수 있을까?&lt;/li&gt;
  &lt;li&gt;자원 관리(Resource management) : 컴퓨터 클러스터를 만드는 것은 엄청나게 비싸기 때문에 많은 유저들에게 공유됩니다. 자원 사용률을 최대화 해서 모두의 요구를 만족시키려면 어떻게 클러스터를 관리하고 자원을 할당해야 할까?&lt;/li&gt;
  &lt;li&gt;프로그래밍 모델(Programming model) : 평소에 프로그래밍 한 것과 같은 방법으로 분산 머신러닝 프로그래밍 해야할까? 코드량을 줄이고 효율성을 향상시키는 새로운 프로그래밍 모델을 만들 수 있을까? 하나의 노드에서 프로그래밍 한 것과 같이 프로그래밍 하면 자동으로 확장시킬 수 있을까?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;대부분의 주류 머신러닝 소프트웨어들은 이러한 기술들에 집중하고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;딥러닝-분산-학습에-대한-이해&quot;&gt;딥러닝 분산 학습에 대한 이해&lt;/h2&gt;
&lt;p&gt;딥러닝 분산 학습은 최근 다양한 분야에서 성과를 거두며 굉장히 중요해진 머신러닝 분산 학습의 작은 분야입니다. 딥러닝 분산 학습의 핵심이나 직면하고 있는 문제점들로 들어가기 전에 몇가지 알아야 할 용어들이 있습니다. 바로 데이터 병렬화(data parallelism)과 모델 병렬화(model parallelism) 입니다.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;데이터-병렬화data-parallelism&quot;&gt;데이터 병렬화(Data parallelism)&lt;/h3&gt;
&lt;p&gt;데이터 병렬화(&lt;a href=&quot;https://en.wikipedia.org/wiki/Data_parallelism&quot;&gt;Data parallelism&lt;/a&gt;)는 데이터를 쪼개서 병렬성을 가능하게 하는 기술입니다. 데이터 병렬화를 사용하면 먼저 데이터를 worker machines(computational node)의 수 만큼 나눕니다. 그 다음 우리는 각 worker가 하나의 독립적인 조각을 갖게 하고 그 데이터에 대해 연산을 하도록 합니다. 우리는 병렬적으로 데이터를 읽는 여러개의 노드를 갖고 있기 때문에 하나의 노드를 사용할 때보다 더 많은 데이터를 읽을 수 있을 것입니다. 데이터 병렬화를 통해 처리량을 증가시킨 것입니다.&lt;/p&gt;

&lt;p&gt;여러개의 노드를 사용해 수렴 속도를 높이고자 하는 분산 딥러닝에서, 데이터 병렬화는 직관적입니다. 우리는 각각의 워커가 자신의 데이터 조각에 대해 학습(경사 하강법)을 진행하도록 하고 그것에 대해 파라미터 업데이트(gradient)를 하도록 합니다. 모든 노드들이 네트워크를 통해 파라미터 상태들을 동기화시켜 모두 같은 값을 갖도록 합니다. 동기화를 하는데 시간이 지나치게 오래 걸리지 않는 한 하나의 노드를 사용할 때보다 향상된 결과를 볼 수 있을 것입니다. 이 방법이 구글의 초기 딥러닝 시스템인 &lt;a href=&quot;https://en.wikipedia.org/wiki/TensorFlow#DistBelief&quot;&gt;DisBelief&lt;/a&gt;가 본질적으로 작동하는 방식입니다.&lt;/p&gt;

&lt;h3 id=&quot;모델-병렬화model-parallelism&quot;&gt;모델 병렬화(Model parallelism)&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;데이터 병렬화와 비교했을때, 모델 병렬화는 더 복잡하고 추상적인 개념입니다. 일반적으로, 모델 병렬화에서는 데이터가 아닌 모델을 여러개의 워커에 나눕니다. 예를 들어 우리가 행렬 인수분해(Matrix factorization)를 하려고 할 때, 행렬의 크기가 너무 크고 우리는 이 거대한 행렬의 모든 파라미터를 알고 싶다고 가정해봅시다. 모델 병렬화를 진행하기 위해 우리는 행렬을 작은 단위(부분 행렬)로 나누고 각각의 워커에게 나눠줄 것입니다. 하나의 워커에 있는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Random-access_memory&quot;&gt;RAM&lt;/a&gt;이 행렬의 파라미터를 담기에 충분하지 않다면 이 방법으로 여러 노드의 추가적인 램을 사용할 수 있게 됩니다. 다양한 노드들이 각각 행렬의 다른 부분들에 해당하는 일을 처리하기 때문에, 병렬적으로 계산할 때 속도 향상을 얻을 수 있게됩니다.&lt;/p&gt;

&lt;p&gt;여기서 모델을 어떻게 나눠야 할까? 라는 질문이 떠오르게 됩니다. 너무나도 다양한 머신러닝 모델들이 있고 각 모델들마다 성격과 특징이 또 다르기 때문에, 모델 병렬화를 구현하는 원칙적 방법은 없습니다.&lt;/p&gt;

&lt;h3 id=&quot;분산-딥러닝의-문제들&quot;&gt;분산 딥러닝의 문제들&lt;/h3&gt;
&lt;p&gt;데이터 병렬화는 학습 데이터의 양이 많아질 때 더 빠르게 데이터를 읽을 수 있어서 상당히 효과적입니다. 모델 병렬화는 모델의 크기가 하나의 노드로 처리하기에 너무 클 때 여러 노드들의 메모리를 사용할 수 있게 해주므로 잘 들어 맞습니다.&lt;/p&gt;

&lt;p&gt;이상적으로 분산 딥러닝에서 우리가 할당한 머신의 수만큼의 속도향상을 얻길 원합니다.(보통 확장성이란 지표로 부릅니다.) 조금 더 구체적으로 K개의 머신을 할당했다면, 우리의 시스템이 하나의 머신에서 작동할 때보다 K배 더 빠르게 작동한다면 K의 확장성 혹은 선형적 확장성을 가지고 있다고 말합니다. 이러한 시스템에서 선형적 확장성은 이상적인 목표입니다.&lt;/p&gt;

&lt;p&gt;하지만 동기화로 인한 오버헤드 때문에, 보통 하나의 노드에서 학습할 때보다 분산 컴퓨터 클러스터에서 학습할 때 더 오랜 시간이 걸립니다. 우리는 머신러닝 태스크의 수렴을 위해 계산이 끝날 때마다 여러 노드들에 대해 동기화를 하는데 추가로 시간을 사용해야 합니다. 실제로 동기화는 계산만큰 시간이 걸리거나 더 걸리기도 합니다.&lt;/p&gt;

&lt;p&gt;왜일까요? 가장 큰 이유는 다른 노드들보다 느리게 작동하는 특정 노드들이 있기 때문입니다. 그들을 동기화시키기 위해 빠른 노드가 느린 노드들이 일을 끝낼 때까지 기다려야 합니다. 시스템 성능은 항상 더 느린 노드들에 의해 결정됩니다. 이러한 경우에 K개의 머신을 같이 놓는 것은 음의 확장성을 보일 것이고 이것은 돈과 시간 낭비입니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;각주&quot;&gt;각주&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;더 공부해보면 좋을 논문입니다.&lt;br /&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~seunghak/SSPTable_NIPS2013.pdf&quot;&gt;More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;아래 논문들을 추천해드립니다.&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1311.4780&quot;&gt;Asymptotically Exact, Embarrassingly Parallel MCMC, by Willie Neiswanger&lt;/a&gt;, Chong Wang, Eric P. Xing. UAI 2014.&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.1576&quot;&gt;LightLDA: Big Topic Models on Modest Compute Clusters&lt;/a&gt;, by Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang Wei, Xun Zheng, Eric P. Xing, Tie-yan Liu, Wei-Ying Ma. WWW 2015.&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.02496&quot;&gt;SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs&lt;/a&gt;, by Kaiwei Li, Jianfei Chen, Wenguang Chen, Jun Zhu. ASPLOS 2017. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;두개의 주목할만한 큰 규모의 분산 딥러닝 논문이 NIPS 2012와 2013 ICML에서 출판되었습니다.&lt;br /&gt;첫번째 논문은 구글의 내부 딥러닝 프레임워크의 첫번째 세대에 관해서 설명하고 있습니다. 두번째 논문은 실리콘밸리의 바이두를 이끌고 있는 Adam Coates가 쓴 논문입니다. 이 두 논문의 핵심 아이디어는 더 많은 연산 노드를 사용해 딥러닝 연산의 규모를 키우는 것입니다. 첫번째 논문은 데이터 병렬화 두번째 논문은 모델 병렬화를 사용했습니다.&lt;br /&gt;&lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf&quot;&gt;Large Scale Distributed Deep Networks&lt;/a&gt;, by Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, Andrew Y. Ng.&lt;br /&gt;&lt;a href=&quot;http://proceedings.mlr.press/v28/coates13.pdf&quot;&gt;Deep Learning with COTS HPC&lt;/a&gt;, Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, Andrew Ng. ICML 2013. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;모델 병렬화에 특히 관심 있으신 분들을 다음 두 논문들을 보시면 좋으실 겁니다.&lt;br /&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~epxing/papers/2016/Kim_etal_EuroSys16.pdf&quot;&gt;STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning&lt;/a&gt;, by Jin Kyu Kim, Qirong Ho, Seunghak Lee, Xun Zheng, Wei Dai, Garth A. Gibson, Eric P. Xing. EuroSys 2016.&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.04972&quot;&gt;Device Placement Optimization with Reinforcement Learning&lt;/a&gt;, by Azalia Mirhoseini Lt ; / RTI &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Jinho Lee</name><email>elzinomaster@gmail.com</email></author><category term="deep learning" /><category term="distributed deep learning" /><summary type="html">Intro to Distributed Deep Learning Systems를 번역한 문서입니다.</summary></entry><entry><title type="html">2019 건양 헬스 데이터톤 우승 후기</title><link href="https://elzino.github.io/projects/2019-12-01/konyang-health-datathon" rel="alternate" type="text/html" title="2019 건양 헬스 데이터톤 우승 후기" /><published>2019-12-01T00:00:00+00:00</published><updated>2019-12-01T00:00:00+00:00</updated><id>https://elzino.github.io/projects/2019-12-01/konyang-health-datathon</id><content type="html" xml:base="https://elzino.github.io/projects/2019-12-01/konyang-health-datathon">&lt;p&gt;지난 11월 29일 부터 11월 30일까지 무박 2일로 진행된 &lt;a href=&quot;https://github.com/khd2019/khd2019&quot;&gt;건양 헬스 데이터톤&lt;/a&gt;(Konyang Health Datathon 2019)에 참가했습니다! 이번에도 친구인 승일이와 ‘행복코딩’팀으로 참여했는데요. 이번 대회는 안저 이미지를 정상 안저, 황반 변성, 당뇨성 망막 병증, 망막 정맥 폐쇄 이미지로 분류하는 A조와 유방 촬영 이미지를 양성과 악성으로 분류하는 B조로 나뉘어서 진행되었습니다. 저희는 A조에 참여해 당당히 1등을 하였습니다!! :clap::clap::clap: 대회를 어떻게 준비했는지, 또 어떤걸 느꼈는지 간단히 기록해보려고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/jvg9rfs336liz4z/leaderboard.png?raw=1&quot; alt=&quot;리더보드&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;사전-조사&quot;&gt;사전 조사&lt;/h2&gt;
&lt;p&gt;안저 이미지 분류는 원래 잘 알지 못하던 분야였습니다. 그런데 조사를 해보니 4년전에 &lt;a href=&quot;https://www.kaggle.com/c/diabetic-retinopathy-detection&quot;&gt;Kaggle에서 주관한 대회&lt;/a&gt;도 열렸었고 구글도 &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45732.pdf&quot;&gt;당뇨성 망막 병증을 분류하는 논문&lt;/a&gt;을 냈었습니다. 그래서 대회전에 미리 Kaggle 대회에서 상위팀들의 솔루션 및 관련 논문들을 읽어봤습니다. 이 중에서 Kaggle 1등팀인 Ben Graham의 &lt;a href=&quot;http://blog.kaggle.com/2015/09/09/diabetic-retinopathy-winners-interview-1st-place-ben-graham/&quot;&gt;Gaussian Blur를 이용해 local average를 빼주는 방법&lt;/a&gt;이 인상 깊었습니다.&lt;/p&gt;

&lt;h2 id=&quot;대회&quot;&gt;대회&lt;/h2&gt;
&lt;p&gt;대회를 가서 당황한 부분은 다시 nsml이였습니다. 대회전에 Pytorch를 이용해 베이스 코드를 미리 짜서 갔었는데 nsml에서 도커 이미지 생성에 삼십분 가량이 걸렸습니다. 반면 기본으로 제공하는 Keras를 이용한 코드를 돌려보니 도커 생성이 약 10초만에 되서 대회장에서 Keras를 쓰기로 방향을 틀었습니다.&lt;br /&gt;
대회를 참여하면서 성능을 올리기 위해 여러가지를 시도했습니다. 먼저 저번 NAVER 대회에서 배운 교훈대로 train set과 valid set을 나누는 데 신경을 썼습니다. Category 별로 비율을 맞춰서 train set과 valid set을 나누었습니다. 모델은 VGG-network를 변형해서 사용했고 Keras에서 제공하는 ImageDataGenerator를 이용해 rotation, zoom, width shift, height shift, flip을 시켜서 Augmentation 했습니다. Optimizer는 RAdam을 사용했고 Label smooth도 적용했습니다. 또한 학습 후반부에는 Augmentation을 거의 주지 않고 learning rate와 label smooth 비율을 낮춰서 fine tuning 한 점이 성능 향상에 큰 도움을 주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;느낀점&quot;&gt;느낀점&lt;/h2&gt;
&lt;p&gt;큰 기대를 하지 않고 참여했던 대회인데 좋은 결과가 있어서 너무 기분이 좋았습니다. Keras도 거의 사용해보지 않았었는데 이번 기회에 다루면서 짧은 시간에 모델을 짤때는 확실히 강점이 있음을 느꼈습니다. 다만 Image classification의 경험이 거의 없어서 여러가지를 시도할 때 과연 이게 성능에 도움이 될까? 라는 의구심이 계속 들었습니다. 이론적 기반 없이 동전 던지기를 잘해 성능이 잘 나온 거 같아 좀 더 경험을 쌓아야 할 것 같습니다. 외부 대회에서 처음으로 1등을 해봐서 잊지 못할 추억이 될 것 같습니다. :+1:&lt;/p&gt;</content><author><name>Jinho Lee</name><email>elzinomaster@gmail.com</email></author><category term="hackathon" /><category term="deep learning" /><category term="medical image classification" /><summary type="html">지난 11월 29일 부터 11월 30일까지 무박 2일로 진행된 건양 헬스 데이터톤(Konyang Health Datathon 2019)에 참가했습니다! 이번에도 친구인 승일이와 ‘행복코딩’팀으로 참여했는데요. 이번 대회는 안저 이미지를 정상 안저, 황반 변성, 당뇨성 망막 병증, 망막 정맥 폐쇄 이미지로 분류하는 A조와 유방 촬영 이미지를 양성과 악성으로 분류하는 B조로 나뉘어서 진행되었습니다. 저희는 A조에 참여해 당당히 1등을 하였습니다!! :clap::clap::clap: 대회를 어떻게 준비했는지, 또 어떤걸 느꼈는지 간단히 기록해보려고 합니다.</summary></entry><entry><title type="html">RAdam : On the Variance of the Adaptive Learning Rate and Beyond</title><link href="https://elzino.github.io/papers/2019-11-21/radam" rel="alternate" type="text/html" title="RAdam : On the Variance of the Adaptive Learning Rate and Beyond" /><published>2019-11-21T00:00:00+00:00</published><updated>2019-11-21T00:00:00+00:00</updated><id>https://elzino.github.io/papers/2019-11-21/radam</id><content type="html" xml:base="https://elzino.github.io/papers/2019-11-21/radam">&lt;p&gt;논문 링크 : &lt;a href=&quot;https://arxiv.org/abs/1908.03265&quot;&gt;arxiv&lt;/a&gt;&lt;br /&gt;
저자 코드 : &lt;a href=&quot;https://github.com/LiyuanLucasLiu/RAdam&quot;&gt;Author’s code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;카카오 블라인드 공채에 지원해서 온라인, 오프라인 코딩테스트를 넘고 나니 회사별 과제가 주어졌습니다. 저는 카카오 브레인에 지원해서, RAdam 논문을 읽고 요약한 뒤 카카오에서 제공한 코드를 개선하라는 과제를 받았습니다. 비록 떨어지긴 했지만(:cry:) 너무나도 재밌는 논문이었습니다. 논문 내용들을 요약해 보았습니다.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;이 논문에서는 기존 Adaptive learning rate 방법들이 bad local optima에 빠지던 문제가 학습초기에 adpative learning rate의 variance가 크기 때문임을 보입니다.&lt;/li&gt;
  &lt;li&gt;또한 warm-up learning rate 방법도 adpative learning rate의 variance를 줄여서 잘 작동하는 것임을 보입니다.&lt;/li&gt;
  &lt;li&gt;마지막으로 Adaptive learning rate의 variance를 일정하게 만들어주는 RAdam이라는 새로운 방법을 제시합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variance-of-adaptive-rate-in-the-early-stage&quot;&gt;Variance of adaptive rate in the early stage&lt;/h2&gt;
&lt;p&gt;저자는 학습 초기에 샘플이 부족하기 때문에 adaptive learning rate가 부적절하게 큰 variance를 갖고, 이것이 안좋은 local optima로 이끈다는 것을 수식적으로, 또 실험적으로 보여줍니다.&lt;/p&gt;

&lt;h3 id=&quot;수식적-분석&quot;&gt;수식적 분석&lt;/h3&gt;
&lt;p&gt;저자는 먼저 time-step 마다 얻어지는 gradient들(\({g_1, \cdots, g_t}\))을 모두 정규분포 \( \mathcal{N}(0, \sigma^2)\)에서 i.i.d 하게 뽑힌 random variables 이라고 가정합니다. 가정에 의해 \( \frac{1}{g_1^2} \)은 the scaled inverse chi-squared distribution을 따르게 됩니다. 이때 sample이 1개인 첫번째 adaptive learning rate의 분산을 계산하면 다음의 식이 나옵니다. \[ \mathrm{Var}[\sqrt{\frac{1}{g_1^2}}] \propto \int_0^{\infty} x^{-1} e^{-x} dx \] 이 식을 통해 sample이 1개일 때 adaptive learning rate의 분산이 발산하는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;실험적-분석&quot;&gt;실험적 분석&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/fr7w6ghn0moin91/result1.jpg?raw=1&quot; alt=&quot;실험이미지1&quot; width=&quot;600px&quot; /&gt;
저자는 Adam을 warm-up을 적용한 채로, 또 다른 한번은 warm-up을 적용 안한채로 각각 학습하며 iteration마다 gradient 값의 측정해보았습니다. x축은 gradient의 절댓값을 log scale로 나타내었고 그래프의 높이는 frequency를 나타냅니다. 그리고 왼쪽 그래프는 iteration 1번째부터 100번까지의 결과, 오른쪽 그래프는 7만번까지의 결과를 보여줍니다.&lt;br /&gt;
실험 결과를 보면 warm-up 없이 학습한 경우 iteration이 10번도 되기 전에 gradient 값들이 상대적으로 작은 값으로 변형되는 것을 볼 수 있습니다. 이러한 결과는 초기에 bad/suspicious local optima에 빠진다는 것을 의미합니다. warm-up은 초기의 작은 update로 초기에 bad/suspicious local optima에 빠지는 것을 방지합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/tne3ged8gvvjhg3/result2.jpg?raw=1&quot; alt=&quot;실험이미지2&quot; /&gt;
저자는 또한 Adam-2k, RAdam, Adam-eps로도 위 실험을 반복합니다.&lt;br /&gt;
Adam-2k는 처음 2000번 update동안 parameter를 update 시키지 않고 adaptive learing rate만을 계산하는 방법입니다. 따라서 다른 방법들과 비교를 위해 iteration이 -1999번부터 시작된다고 하고 실험을 진행하였습니다. 그 결과 bad local optima에 빠지지 않는 것을 확인 할 수 있습니다. 이는 저자의 가설에서 Adam이 초기에 sample이 부족한 것이 문제의 원인이라는 것을 입증합니다.&lt;br /&gt;
Adam-eps는 adaptive learning rate 식인 \(\hat{\psi}(g_1, \cdots, g_t) = \frac{\sqrt{1-\beta_2^t}}{\epsilon + \sqrt{(1 - \beta_2)\sum_{i = 1}^t \beta_2^{t-i}g_i^2}}\) 에서 엡실론 값을 키워 variance를 낮추는 방법입니다. 이 결과 역시 초기의 bad local optima에 빠지지 않는 것을 보입니다.&lt;/p&gt;

&lt;h2 id=&quot;analysis-of-adaptive-learning-rate-variance&quot;&gt;Analysis of adaptive learning rate variance&lt;/h2&gt;
&lt;p&gt;이번에는 adaptive learning rate의 variance를 구해보고 sample의 수가 늘어날수록 variance가 감소함을 보입니다. 이때 분석의 용이함을 위해 저자는 총 3번의 근사를 시킵니다.  &lt;br /&gt;
첫번째 근사는 exponential moving average를 simple average로 근사시키는 것입니다. 저자는 exponential moving average가 simple average보다 더 큰 분산을 갖을 뿐더러 t가 작을때는 simple average와 차이도 상대적으로 작으므로 이 근사가 무리가 없다고 주장합니다. 
\[p(\psi(.)) = p(\sqrt{\frac{1-\beta_2^t}{(1 - \beta_2)\sum_{i = 1}^t \beta_2^{t-i}g_i^2}}) \approx p(\sqrt{\frac{t}{\sum_{i=1}^t g_i^2}}) \]
\( g_i \sim \mathcal{N}(0, \sigma^2) \)을 가졍했기 때문에 \( \frac{t}{\sum_{i=1}^t g_i^2} \sim Scale–inv–\mathcal{X}^2(t,\frac{1}{\sigma^2}) \) 입니다. 근사 시킨 값이 scaled inverse chi-square 분포를 따르므로 \(\frac{1-\beta_2^t}{(1 - \beta_2)\sum_{i = 1}^t \beta_2^{t-i}g_i^2}\) 또한 자유도 \( \sigma \)를 갖는 scaled inverse chi-square 분포로 가정하겠습니다. 이 가정하에서 저자는 분산을 구합니다.
&lt;img src=&quot;https://www.dropbox.com/s/2dm83bbd7op9t07/Theorem%201.png?raw=1&quot; alt=&quot;Theorem 1&quot; /&gt;
여기서 \( \mathcal{B}(.) \)은 Beta function을 의미합니다. 또한 \( \mathrm{Var}[\psi(.)] \)의 미분 값을 통해 자유도 \( \sigma \)가 증가함에 따라 분산이 단조 감소하는 것을 보입니다. 이로써 학습 초기에 sample이 부족한 것 때문에 나중보다 분산이 크다는 것을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;rectified-adaptive-learning-rate&quot;&gt;Rectified adaptive learning rate&lt;/h2&gt;
&lt;p&gt;저자는 앞에서 구한 계산값을 이용해 adaptive learning rate의 분산을 일정하게 만들어주는 방법을 제시합니다. 이를 위해서 먼저 자유도인 \( \sigma \)를 \( t \)를 이용해 구합니다.&lt;/p&gt;

&lt;h3 id=&quot;estimation-of--sigma-&quot;&gt;Estimation of \( \sigma \)&lt;/h3&gt;
&lt;p&gt;자유도를 구하기 위해 두번째 근사를 합니다. 경제학에서 exponential moving average(EMA)는 simple moving average(SMA)로 자주 근사된다고 합니다.
\[ p(\frac{(1 - \beta_2)\sum_{i = 1}^t \beta_2^{t-i}g_i^2}{1-\beta_2^t}) \approx p(\frac{\sum_{i = 1}^{f(t, \beta_2)} g_{t+1-i}^2}{f(t, \beta_2)}) \]
이때 \( f(t, \beta_2) \)는 SMA의 길이로 SMA가 EMA와 같은 “center of mass”를 갖도록 구해집니다. 따라서 \(f(t, \beta_2) = \frac{2}{1 - \beta_2} - 1 - \frac{2 t \beta_2^t}{1 - \beta_2^t}\) 입니다.
\[ \frac{1-\beta_2^t}{(1 - \beta_2)\sum_{i = 1}^t \beta_2^{t-i}g_i^2} \approx \frac{f(t, \beta_2)}{\sum_{i=1}^{f(t, \beta_2)} g_i^2} \sim Scale–inv–\mathcal{X}^2(\rho,\frac{1}{\sigma^2}) \]
\[ \rho \approx f(t, \beta_2) = \frac{2}{1 - \beta_2} - 1 - \frac{2 t \beta_2^t}{1 - \beta_2^t} \]
위와 같이 \( \rho \)의 근사값으로 \( f(t, \beta_2) \)을 이용해 \( t \)에 대한 함수로 표현할 수 있게 되었습니다. 추가로 \( \rho_t = f(t, \beta_2) \)라고 표기했을 때 \( \rho_\infty = \frac{2}{1 - \beta_2} - 1 \)가 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;variance-estimation-and-rectification&quot;&gt;Variance estimation and rectification&lt;/h3&gt;
&lt;p&gt;위에서 분산은 자유도 \( \rho \)가 증가함에 따라 단조감소함을 보였으므로 \( \min_{\rho_t} \mathrm{Var}[\psi(.)] = \mathrm{Var}[\psi(.)]|_ {\rho_t = \rho_\infty} \)이고 이때의 분산을 \( C_{\mbox{var}} \)라고 표기하겠습니다. 저자는 학습 초기의 분산이 커서 bad local optima에 빠지는 문제를 방지하기 위해 매 timestep마다 분산이 \( C_{\mbox{var}} \)가 되도록 아래와 같이 rectification 해줍니다.
\[
  \mathrm{Var}[r_t \,\psi(g_1, \cdots, g_t)] = C_{\mbox{var}}
  \quad
  \mbox{where}
  \quad
  r_t = \sqrt{\frac{C_{\mbox{var}}}{\mathrm{Var}[\psi(g_1, \cdots, g_t)]}}
\]
위에서 \( \mathrm{Var}[\psi(.)] \) 값을 구했지만 수치적으로 불안정하므로 \( \sqrt{\psi^2(.)} \) 에 대한 1차 근사(세번째 근사)를 이용해서 다시 구하면 다음과 같습니다.
\[ \mathrm{Var}[\psi(.)] \approx \frac{\rho_t}{2(\rho_t - 2)(\rho_t - 4)\sigma^2} \]
분산을 \( C_{\mbox{var}} \)로 일정하게 유지시키는 \( r_t \) 값은 다음과 같습니다.
\[
r_t = \sqrt{\frac{(\rho_t - 4)(\rho_t - 2)\rho_\infty}{(\rho_\infty - 4)(\rho_\infty - 2)\rho_t}}
\]
단 \(\rho_t\)가 4보다 작을 경우 adaptive learning rate는 무시합니다. 이를 정리해 적용한 알고리즘은 다음과 같습니다.
&lt;img src=&quot;https://www.dropbox.com/s/lnmad7y7hac01q7/algorithm.png?raw=1&quot; alt=&quot;Algorithm 2&quot; /&gt;
위와 같이 Adam의 분산을 일정하게 만들어주는 방법을 통해 RAdam이라는 새로운 방법을 제안합니다.&lt;/p&gt;
&lt;h3 id=&quot;in-comparison-with-warmup&quot;&gt;In comparison with warmup&lt;/h3&gt;
&lt;p&gt;저자는 \( r_t \)가 결국 gradient에 \( \frac{min(t, T_w)}{T_w} \)을 곱하는 heuristic linear warmup과 비슷한 역할을 한다고 말합니다. 하지만 RAdam은 \( T_w \)와 같은 hyperparameter를 추가로 필요하지 않는 장점이 있습니다. 또한 뒤에서 살펴볼 실험 결과에 따르면 Adam-warmup은 learning rate를 증가시킬 step 수에 따라 learning rate에 민감함을 보이고, RAdam은 robust함을 보입니다.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;저자는 Language Modeling을 평가하기 위해 One Billion Word 데이터셋을, Image Classification을 평가하기 위해 Cifar10 와 ImageNet 데이터셋을 사옹했습니다.&lt;/p&gt;

&lt;h3 id=&quot;radam-vs-vanila-adam-vs-sgd&quot;&gt;RAdam vs Vanila Adam vs SGD&lt;/h3&gt;
&lt;p&gt;저자는 먼저 vanila Adam과 비교해 RAdam이 더 좋은 성능을 내는 것을 보입니다.
&lt;img src=&quot;https://www.dropbox.com/s/fbrghwh9tjiddyq/result3.png?raw=1&quot; alt=&quot;Result1&quot; /&gt;
또한 learning rate의 변화에 robust함을 보입니다.
&lt;img src=&quot;https://www.dropbox.com/s/1u2gbp19lu981zm/result4.png?raw=1&quot; alt=&quot;Result2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;radam-vs-adam-warmup&quot;&gt;RAdam vs Adam-warmup&lt;/h3&gt;
&lt;p&gt;RAdam은 위에서 말했듯이 Adam-warmup과 달리 처음에 learning rate를 증가시킬 step수를 지정하지 않아도 되므로 hyper-parameter가 하나 더 적습니다. 또한 Adam-warmup은 learning rate를 증가시킬 step 수에 따라 learning rate에 민감함을 보이고, RAdam은 robust함을 보입니다.
&lt;img src=&quot;https://www.dropbox.com/s/l6az85a1i2r8mep/result5.png?raw=1&quot; alt=&quot;Result3&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;느낀점&quot;&gt;느낀점&lt;/h2&gt;
&lt;p&gt;optimization에 관련된 논문은 처음으로 읽어봤습니다. Adam이 나온 이후로 모든 task에 general하게 잘 작동하는 optimizer가 나오지 않은것으로 알고 있습니다. 새로운 시각으로 접근해서 재밌었고 논문에서 이론적 증명을 통해 RAdam의 정당성을 보여준 점이 좋았습니다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b&quot;&gt;New State of the Art AI Optimizer: Rectified Adam (RAdam). - Less Wright&lt;/a&gt;&lt;/p&gt;</content><author><name>Jinho Lee</name><email>elzinomaster@gmail.com</email></author><category term="deep learning" /><category term="optimizer" /><summary type="html">논문 링크 : arxiv 저자 코드 : Author’s code 카카오 블라인드 공채에 지원해서 온라인, 오프라인 코딩테스트를 넘고 나니 회사별 과제가 주어졌습니다. 저는 카카오 브레인에 지원해서, RAdam 논문을 읽고 요약한 뒤 카카오에서 제공한 코드를 개선하라는 과제를 받았습니다. 비록 떨어지긴 했지만(:cry:) 너무나도 재밌는 논문이었습니다. 논문 내용들을 요약해 보았습니다.</summary></entry><entry><title type="html">2019 NAVER AI HACKATHON SPEECH 참가 후기</title><link href="https://elzino.github.io/projects/2019-11-16/naver-ai-hackathon-review" rel="alternate" type="text/html" title="2019 NAVER AI HACKATHON SPEECH 참가 후기" /><published>2019-11-16T00:00:00+00:00</published><updated>2019-11-16T00:00:00+00:00</updated><id>https://elzino.github.io/projects/2019-11-16/naver-ai-hackathon-review</id><content type="html" xml:base="https://elzino.github.io/projects/2019-11-16/naver-ai-hackathon-review">&lt;p&gt;2019년 9월 중순부터 10월 중순까지 약 한 달가량 진행된 &lt;a href=&quot;https://campaign.naver.com/aihackathon_speech/&quot; target=&quot;_blank&quot;&gt;NAVER AI HACATHON&lt;/a&gt;에 참여하였습니다! 이번 해커톤의 주제는 한국어 음성인식으로 네이버에서 공개한 50000쌍의 한국어 전화망 데이터를 이용해 진행하였습니다. 대학교 친구인 &lt;a href=&quot;https://github.com/psi9730&quot; target=&quot;_blank&quot;&gt;박승일&lt;/a&gt;군과 함께 행복코딩 팀으로 참여해 100팀 중 9등이라는 성과를 거두었습니다! 와아~ :clap: 처음 참가해보는 해커톤이였고 음성인식분야도 잘 몰랐었는데 좋은 성적을 거두어서 기분 좋게 마무리 할 수 있었습니다. 참가신청부터 결선까지 어떻게 준비하고 참여하였는지 후기로 남겨보려고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;배경지식-탐색&quot;&gt;배경지식 탐색&lt;/h2&gt;
&lt;p&gt;서류 통과 메일을 받은 후부터 본격적으로 Speech Recognition 분야를 survey 해보았습니다.
제일 먼저 이 부분의 SOTA를 찾아보기위해 &lt;a href=&quot;https://paperswithcode.com/task/speech-recognition&quot; target=&quot;_blank&quot;&gt;Paper with code&lt;/a&gt; 사이트에서 Dataset별로 어떤 연구가 진행되었는지 찾아보았습니다.(Paper with code 짱짱 ㅎㅎ)&lt;br /&gt;
그 후 우선적으로 다음 논문들을 읽어보았습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.5567&quot; target=&quot;_blank&quot;&gt;Deep Speech&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.02595&quot; target=&quot;_blank&quot;&gt;Deep Speech 2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.01211&quot; target=&quot;_blank&quot;&gt;Listen, Attend and Spell&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot; target=&quot;_blank&quot;&gt;SpecAugment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;또한 Deep Speech를 만든 바이두에서 강연한 &lt;a href=&quot;https://www.youtube.com/watch?v=9dXiAecyJrY&amp;amp;feature=youtu.be&amp;amp;t=13874&quot; target=&quot;_blank&quot;&gt;영상&lt;/a&gt;이 있는데 기존의 전통적인 Speech Recognition 방식에서 벗어나, 어떻게 딥러닝을 사용해 성공적으로 성능을 끌어 올렸는지 쉽게 설명해주십니다. CTC loss, Language Model 등 Deep Speech에서 사용된 개념뿐만 아니라 음성인식 분야에 전반적인 이해를 돕는 영상이라 처음 이 분야를 공부하시려는 분들한테 강추드립니다! :thumbsup:&lt;br /&gt;
Deep Speech, Deep Speech 2는 CTC loss를 사용하였고 Listen, Attend and Spell은 encoder-decoder 방식을 사용하였습니다. 저는 survey 당시 SOTA였던 SpecAugment 방식을 사용하고 싶었고 Naver에서 제공한 &lt;a href=&quot;https://github.com/clovaai/speech_hackathon_2019&quot; target=&quot;_blank&quot;&gt;baseline code&lt;/a&gt;도 encoder-decoder 방식을 사용하고 있었기 때문에 encoder-decoder 방식을 사용하기로 결정하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;예선&quot;&gt;예선&lt;/h2&gt;
&lt;h4 id=&quot;nsml&quot;&gt;NSML&lt;/h4&gt;
&lt;p&gt;예선이 시작하고 제일 당황스러웠던 것은 로컬에서 잘 돌아가는 모델이 NSML(Naver에서 제공한 딥러닝용 클라우드)에 올리면 잘 안돌아가는 것이였습니다. Librosa 라이브러리를 사용해 mel-spectrogram으로 변환하려고 하였는데 segmentation 오류가 나면서 세션이 죽어버리곤 했습니다. 아무리 디버깅을 해도 이유를 알 수 없어 포기하고 torchaudio를 쓰려던 차에 library간 버전 충돌 때문임을 알고 docker 설정을 바꿔 해결하였습니다.&lt;/p&gt;
&lt;h4 id=&quot;모델-구조-수정-data-전처리-label-smooth&quot;&gt;모델 구조 수정, Data 전처리, label smooth&lt;/h4&gt;
&lt;p&gt;예선 초반에 Listen, Attend and Spell 방식으로 encoder-decoder에 attention을 달고 테스트 해보니 6등으로 출발했습니다. 본선 진출은 문제 없겠다라고 생각했는데 그 이후로 시도하는 것마다 성능 향상을 보이지 못해 28등까지 떨어졌습니다. 이때 이번 해커톤에서 결과를 평가할때 공백을 제거하고 비교하는 점에서 착안해 모델이 아예 공백을 예측하지 않도록 바꾸니 큰 성능향상이 이뤄져서 등수를 많이 올릴 수 있었습니다. 추가로 label smooth, 데이터 전처리(특수문자 제거) 등을 통해서 성능을 더 올려 예선을 8위로 마무리하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;결선&quot;&gt;결선&lt;/h2&gt;
&lt;p&gt;결선은 온라인 결선과 오프라인 결선으로 나누어 진행되었습니다. 온라인 결선에서는 예선에서 제공되었던 3만쌍에 데이터에다 추가로 2만쌍의 데이터가 더 공개되었습니다.&lt;/p&gt;

&lt;h4 id=&quot;오프라인-결선&quot;&gt;오프라인 결선&lt;/h4&gt;
&lt;p&gt;오프라인 결선은 춘천에 있는 네이버 커넥트원 건물에서 진행되었습니다. 처음 들어가자마자 토니 스타크가 살 것 같다고 말했을 만큼 건물이 깔끔하고 멋있게 지어져 있더라고요
&lt;img src=&quot;https://www.dropbox.com/s/vmeb4rs78p8kuo5/%EC%9B%B0%EC%BB%B4%EA%B8%B0%ED%94%84%ED%8A%B8.jpg?raw=1&quot; alt=&quot;네이버 웰컴 기프트&quot; /&gt;
처음 갔을때 받은 명찰과 웰컴 기프트에요! 저 초록색 동그란 건 손목 운동할 때 쓰는 거더라고요.&lt;br /&gt;
&lt;img src=&quot;https://www.dropbox.com/s/mw2thubrnflbt3f/snack.jpg?raw=1&quot; alt=&quot;간식&quot; /&gt;
해커톤 기간동안 무제한으로 먹을 수 있던 간식입니다! 그런데 저 간식 말고도 중간에 떡볶이, 튀김, 콜팝, 치즈볼등 맛있는 간식들을 너무 많이 주셔서 저것들은 별로 안먹었던거 같아요!&lt;br /&gt;
&lt;img src=&quot;https://www.dropbox.com/s/imvo1l9mnjdw35i/lunch.jpg?raw=1&quot; alt=&quot;점심&quot; /&gt;
&lt;img src=&quot;https://www.dropbox.com/s/8uijyxtfsph16pc/dinner.jpg?raw=1&quot; alt=&quot;저녁&quot; /&gt;
점심과 저녁입니다. 정말 정말 맛있었어요 ㅎㅎ 역시 대기업인가 싶더라고요. 덕분에 맛있게 먹고 해커톤에 집중할 수 있었습니다!&lt;br /&gt;
&lt;img src=&quot;https://www.dropbox.com/s/iu4x8s85yq0n4wi/dorm.jpg?raw=1&quot; alt=&quot;숙소&quot; /&gt;
숙소도 너무 좋았어요. 호텔에 와있는 느낌 ㅎㅎ 클로바 스피커도 처음 사용해 봤는데 너무 신기하더라고요. 이번 해커톤 주제가 음성인식이라 더 신기했던거 같아요.&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/elzino/naver_ai_hackathon_speech/master/docs/final-board.png&quot; alt=&quot;랭킹&quot; /&gt;&lt;br /&gt;
오프라인 결선 기간 동안 어떻게 모델을 개선할지 고민을 많이 했어요. 결론적으로 예선 기간동안 구현했지만 이상하게 성능이 오르지 않았던 빔서치를 좀 더 살펴보았습니다. 그러던 중 빔서치 로직에서 버그를 찾을 수 있었고 버그를 고치니 인식률이 4퍼센트 가량 올랐습니다! 결국 오프라인 결선 시작할 때 11등까지 떨어졌었는데 다시 올려 9등으로 마무리하였습니다. ㅎㅎ&lt;/p&gt;

&lt;p&gt;오프라인 결선이 끝나고 1, 2, 3등 팀과 질의 응답시간이 있었습니다. librosa를 이용해 음성파일 앞, 뒤에 있는 침묵을 제거했다는 점, naver에서 weight initialization 관련해서 함정코드를 심어놓았다는 점이 인상깊었습니다. 그리고 무엇보다 1등팀이 training set, valid set을 나눌 때 화자, 스크립트를 고려해서 나눈점이 인상깊었습니다. 해커톤 기간 내내 validation error 와 test error가 비례하지 않아서 이상하다고 생각했는데 화자 및 스크립트에 overfitting 된 점이 문제였습니다. 기본이지만 간과하고 있던 부분이였습니다.&lt;/p&gt;

&lt;h2 id=&quot;느낀점&quot;&gt;느낀점&lt;/h2&gt;
&lt;p&gt;이번 해커톤에 참여하여서 처음으로 딥러닝 분야 중 한 task에 몰두해서 모델을 학습시키고 개선해보았습니다. 스피치 분야의 매력도 새롭게 알게 되었고 관심을 갖게 되었습니다.&lt;br /&gt;
또한 성능향상을 위해 모델에만 집중하였는데 결국 모델을 향상시키기 위해선 데이터를 잘 이해하는것이 우선인 것을 다시 한 번 깨닫게 되었습니다. weight initialization과 같은 deep learning의 기본의 중요성도 다시 깨닫게 되었습니다.&lt;br /&gt;
제가 많이 성장할 수 있었고 무엇보다 즐겁게 참여하여서 너무 행복한 시간이였습니다.&lt;br /&gt;
좋은 행사를 마련해준 네이버측에 감사를 표합니다!&lt;/p&gt;

&lt;p&gt;추가로 저희팀의 코드 레포지토리 링크를 첨부합니다&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/elzino/naver_ai_hackathon_speech&quot; target=&quot;_blank&quot;&gt;행복코딩팀의 hackathon code link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jinho Lee</name><email>elzinomaster@gmail.com</email></author><category term="hackathon" /><category term="deep learning" /><category term="speech recognition" /><summary type="html">2019년 9월 중순부터 10월 중순까지 약 한 달가량 진행된 NAVER AI HACATHON에 참여하였습니다! 이번 해커톤의 주제는 한국어 음성인식으로 네이버에서 공개한 50000쌍의 한국어 전화망 데이터를 이용해 진행하였습니다. 대학교 친구인 박승일군과 함께 행복코딩 팀으로 참여해 100팀 중 9등이라는 성과를 거두었습니다! 와아~ :clap: 처음 참가해보는 해커톤이였고 음성인식분야도 잘 몰랐었는데 좋은 성적을 거두어서 기분 좋게 마무리 할 수 있었습니다. 참가신청부터 결선까지 어떻게 준비하고 참여하였는지 후기로 남겨보려고 합니다.</summary></entry></feed>